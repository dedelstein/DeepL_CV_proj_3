{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Potholes/annotated-images/'\n",
    "splits = 'Potholes/splits.json'\n",
    "# path = r'C:/Users/Frederik/Programming/IntroductionToComputerVison/Project_3/Potholes/annotated-images/'\n",
    "# splits = r'C:/Users/Frederik/Programming/IntroductionToComputerVison/Project_3/Potholes/splits.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.ops import RoIAlign\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional as Fs\n",
    "from torchvision import transforms as v2\n",
    "import torchvision.ops as ops\n",
    "import torch\n",
    "import json\n",
    "from xml.etree import ElementTree as ET\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.io import decode_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms import v2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# speed-up using multithreads\n",
    "cv2.setUseOptimized(True)\n",
    "cv2.setNumThreads(8)\n",
    "\n",
    "def selective_search(image_path, num_rects, quality=True):\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    ss.setBaseImage(image)\n",
    "    \n",
    "    if quality:\n",
    "        ss.switchToSelectiveSearchQuality()\n",
    "    else:\n",
    "        ss.switchToSelectiveSearchFast()\n",
    "        \n",
    "    rects = ss.process()\n",
    "\n",
    "    return rects[:num_rects]\n",
    "\n",
    "\n",
    "def show_selective_search(image, rects):\n",
    "    imOut = image.copy()\n",
    "\n",
    "    # itereate over all the region proposals\n",
    "    for _, rect in enumerate(rects):\n",
    "        # draw rectangle for region proposal\n",
    "        x, y, w, h = rect\n",
    "        color = list(np.random.random(size=3) * 256)\n",
    "        cv2.rectangle(imOut, (x, y), (x+w, y+h), color, 2, cv2.LINE_AA)\n",
    "\n",
    "    plt.imshow(imOut[...,::-1])\n",
    "    plt.axis('off')\n",
    "\n",
    "def read_xml(path: str) -> list:  \n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    obj_list = []\n",
    "\n",
    "    for obj in root.iter('object'):\n",
    "\n",
    "        ymin = int(obj.find(\"bndbox/ymin\").text)\n",
    "        xmin = int(obj.find(\"bndbox/xmin\").text)\n",
    "        ymax = int(obj.find(\"bndbox/ymax\").text)\n",
    "        xmax = int(obj.find(\"bndbox/xmax\").text)\n",
    "\n",
    "        bbox = (xmin, ymin, xmax, ymax)\n",
    "        obj_list.append(bbox)\n",
    "    \n",
    "    return obj_list\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def visualize_boxes(images, annotations):\n",
    "    all_images = []\n",
    "    all_overlay = []\n",
    "\n",
    "    for image, annotation in zip(images, annotations):\n",
    "        \n",
    "        overlay = draw_bounding_boxes(image, annotation, width=2)\n",
    "        all_images.append(image)\n",
    "        all_overlay.append(overlay)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(all_images), 2, figsize=(10, len(all_images) * 5))\n",
    "    \n",
    "    for idx, image in enumerate(all_images):\n",
    "        axes[idx, 0].imshow(image.permute(1,2,0))\n",
    "        axes[idx, 0].axis('off')\n",
    "\n",
    "        axes[idx, 1].imshow(all_overlay[idx].permute(1,2,0))\n",
    "        axes[idx, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.02, hspace=0.02)\n",
    "    plt.show()\n",
    "        \n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.convolutional1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 224x224 -> 112x112\n",
    "        )\n",
    "        \n",
    "        self.convolutional2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 112x112 -> 56x56\n",
    "        )\n",
    "        \n",
    "        self.convolutional3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 56x56 -> 28x28\n",
    "        )\n",
    "        \n",
    "        self.convolutional4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 28x28 -> 14x14\n",
    "        )\n",
    "\n",
    "        self.convolutional5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 14x14 -> 7x7\n",
    "        )\n",
    "\n",
    "        self.convolutional6 = nn.Sequential(\n",
    "            nn.Conv2d(1024, 2048, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 7x7 -> 3x3\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fully_connected = nn.Sequential(\n",
    "            nn.Linear(2048 * 3 * 3, 2048), \n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutional1(x)\n",
    "        x = self.convolutional2(x)\n",
    "        x = self.convolutional3(x)\n",
    "        x = self.convolutional4(x)\n",
    "        x = self.convolutional5(x)\n",
    "        x = self.convolutional6(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fully_connected(x)\n",
    "        x = x.squeeze(1) \n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import json\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torchvision.ops as ops\n",
    "# from xml.etree import ElementTree as ET\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import tv_tensors\n",
    "# from torchvision.transforms import v2\n",
    "# from torchvision.io import decode_image\n",
    "# from torchvision.utils import draw_bounding_boxes\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Pothole_Dataloader(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, num_rects = 1000, pos_thresh=0.5, neg_thresh=0.3, size=256, val=False, device=\"cpu\"):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.num_rects = num_rects\n",
    "        self.pos_thresh = pos_thresh\n",
    "        self.neg_thresh = neg_thresh\n",
    "        self.size = size\n",
    "        self.val = val\n",
    "        self.device = device\n",
    "        #These transforms can be changed in the future to a more appropriate augmentation, this is just a proof-of-concept placeholder.\n",
    "        #Resize is unecessary i think\n",
    "        self.transforms = v2.Compose([\n",
    "                            v2.RandomHorizontalFlip(),\n",
    "                            v2.RandomVerticalFlip()\n",
    "                          ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = decode_image(self.image_paths[idx])\n",
    "        boxes = tv_tensors.BoundingBoxes(read_xml(self.mask_paths[idx]), \n",
    "                                        format=\"XYXY\", canvas_size=image.shape[-2:])\n",
    "        \n",
    "        if not self.val:\n",
    "            # Generate region proposals with selective search\n",
    "            regions = ops.box_convert(torch.tensor(selective_search(self.image_paths[idx], num_rects=self.num_rects, quality=False)), \"xywh\", \"xyxy\")\n",
    "            region_boxes = tv_tensors.BoundingBoxes(regions, format=\"XYXY\", canvas_size=image.shape[-2:])\n",
    "            \n",
    "            # Calculate IoU between region proposals and ground-truth boxes\n",
    "            ious = ops.box_iou(region_boxes.to(self.device), boxes.to(self.device))\n",
    "\n",
    "            # Select proposals with IoU >= pos_thresh as positive samples\n",
    "            pos_indices = (ious.max(dim=1)[0] >= self.pos_thresh).nonzero(as_tuple=True)[0]\n",
    "            pos_samples = region_boxes[pos_indices]\n",
    "\n",
    "            # Select proposals with IoU < neg_thresh as background samples\n",
    "            bg_indices = (ious.max(dim=1)[0] < self.neg_thresh).nonzero(as_tuple=True)[0]\n",
    "            num_bg = int(len(pos_samples) * 4)  # Make background samples 80% of total proposals\n",
    "            bg_samples = region_boxes[bg_indices[:num_bg]]\n",
    "\n",
    "            # Concatenate positive and background samples\n",
    "            selected_regions = torch.cat([pos_samples, bg_samples], dim=0)\n",
    "            region_labels = torch.cat([torch.ones(len(pos_samples)), torch.zeros(num_bg)])\n",
    "\n",
    "            # Directly return the untransformed values\n",
    "            return image, boxes, regions, selected_regions, region_labels\n",
    "\n",
    "        else:\n",
    "            regions = selected_regions = region_labels = []\n",
    "        \n",
    "        return image, boxes, regions, selected_regions, region_labels\n",
    "\n",
    "\n",
    "\n",
    "train_mask_list = [path + f for f in json.load(open(splits))['train']]\n",
    "val_mask_list = [path + f for f in json.load(open(splits))['test']]\n",
    "train_img_list = [filename.replace('xml', 'jpg') for filename in train_mask_list]\n",
    "val_img_list = [filename.replace('xml', 'jpg') for filename in val_mask_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 512 #image size \n",
    "batch_size = 16\n",
    "num_workers = 1\n",
    "\n",
    "val_img_list, test_img_list, val_mask_list, test_mask_list = train_test_split(val_img_list, val_mask_list, train_size=.5)\n",
    "\n",
    "trainset = Pothole_Dataloader(train_img_list, train_mask_list, size=size, val=False)\n",
    "valset = Pothole_Dataloader(val_img_list, val_mask_list, size=size, val=True)\n",
    "testset = Pothole_Dataloader(test_img_list, test_mask_list, size=size, val=True)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\n",
    "val_loader= DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, boxes, regions, selected_regions, region_labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_resize(image, box, target_size=(224, 224)):\n",
    "    xmin, ymin, xmax, ymax = box\n",
    "\n",
    "    # Ensure image is in the correct format\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.permute(1, 2, 0).numpy()  # Convert from [C, H, W] to [H, W, C] if tensor\n",
    "\n",
    "    cropped_image = image[int(ymin):int(ymax), int(xmin):int(xmax)]\n",
    "\n",
    "    transform = v2.Compose([\n",
    "        v2.ToPILImage(),  \n",
    "        v2.Resize(target_size),  \n",
    "        v2.ToTensor()  \n",
    "    ])\n",
    "\n",
    "    resized_image = transform(cropped_image)\n",
    "    return resized_image\n",
    "\n",
    "\n",
    "def process_selected_regions(image_resized, selected_regions, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    This function processes the selected regions for a given image, crops out each region,\n",
    "    resizes them, and returns a tensor of cropped and resized regions.\n",
    "    Sometimes there are none in the selected_regions for that image, so that image gets skipped.\n",
    "    \"\"\"\n",
    "    cropped_resized_regions = []\n",
    "\n",
    "    for region in selected_regions:\n",
    "        # Ensure that we have 4 values (xmin, ymin, xmax, ymax) for the region\n",
    "        if region.numel() == 4:\n",
    "            cropped_resized_regions.append(crop_and_resize(image_resized, region, target_size=target_size))\n",
    "        else:\n",
    "            print(f\"Invalid region size for {region}, skipping this region.\")\n",
    "\n",
    "    # Stack the regions into a single tensor (shape: (N, C, 224, 224))\n",
    "    if len(cropped_resized_regions) > 0:\n",
    "        cropped_resized_regions = torch.stack(cropped_resized_regions)\n",
    "    else:\n",
    "        print(\"No valid regions found for this image.\")\n",
    "        cropped_resized_regions = torch.empty(0)  # Return empty tensor if no regions\n",
    "\n",
    "    return cropped_resized_regions\n",
    "\n",
    "\n",
    "def process_images(images, selected_regions, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    This function processes the entire batch from the dataLoader by iterating over all images\n",
    "    and their selected regions, cropping, resizing and returning the processed patches.\n",
    "    \"\"\"\n",
    "    all_cropped_resized_regions = []\n",
    "\n",
    "    for i in range(len(images)):  #\n",
    "        image_resized = images[i].permute(1, 2, 0).numpy()  # [C, H, W] to [H, W, C]\n",
    "        regions = selected_regions[i]  \n",
    "\n",
    "        # Check if selected_regions are empty for the current image\n",
    "        if regions.size(0) == 0:\n",
    "            print(f\"Warning: No selected regions for image {i}\")\n",
    "            continue\n",
    "\n",
    "        # Process the selected regions for the current image\n",
    "        cropped_resized_regions = process_selected_regions(image_resized, regions, target_size)\n",
    "        \n",
    "        # Check if any regions were processed\n",
    "        if cropped_resized_regions.numel() == 0:\n",
    "            print(f\"Warning: No valid regions found for image {i}, skipping this image.\")\n",
    "            continue\n",
    "\n",
    "        all_cropped_resized_regions.append(cropped_resized_regions)\n",
    "\n",
    "    if len(all_cropped_resized_regions) > 0:\n",
    "        return torch.cat(all_cropped_resized_regions, dim=0)\n",
    "    else:\n",
    "        print(\"Warning: No valid regions were found in the batch.\")\n",
    "        return torch.empty(0)  # Return empty tensor if no valid regions in the batch\n",
    "\n",
    "\n",
    "def process_label(images, selected_regions, region_labels):\n",
    "    \"\"\"\n",
    "    This function processes the entire batch from the dataLoader by iterating over all images\n",
    "    and their selected regions, extracting the region labels and returning a flattened tensor of labels.\n",
    "    \"\"\"\n",
    "    all_labels = []\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        labels = region_labels[i]  #\n",
    "\n",
    "        if labels.size(0) == 0:\n",
    "            print(f\"Warning: No labels for image {i}, skipping this image.\")\n",
    "            continue\n",
    "\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    if len(all_labels) > 0:\n",
    "        all_labels_flattened = torch.cat(all_labels)\n",
    "        return all_labels_flattened\n",
    "    else:\n",
    "        print(\"Warning: No valid labels in the batch.\")\n",
    "        return torch.empty(0)  # Return empty tensor if no valid labels\n",
    "\n",
    "\n",
    "# images, boxes, regions, selected_regions, region_labels = next(iter(train_loader))\n",
    "\n",
    "# cropped_resized_regions = process_images(images, selected_regions, target_size=(224, 224))\n",
    "# flattened_labels = process_label(images, selected_regions, region_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cropped_resized_regions.shape)\n",
    "# print(flattened_labels.shape)\n",
    "\n",
    "# small_batches = torch.split(cropped_resized_regions, 32)\n",
    "# len(small_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/34 [00:10<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m  \n\u001b[0;32m     25\u001b[0m total_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m  \n\u001b[1;32m---> 27\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_regions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcropped_resized_regions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprocess_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_regions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflattened_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprocess_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_regions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion_labels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Frederik\\miniconda3\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\Frederik\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Frederik\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Frederik\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[15], line 41\u001b[0m, in \u001b[0;36mPothole_Dataloader.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     36\u001b[0m boxes \u001b[38;5;241m=\u001b[39m tv_tensors\u001b[38;5;241m.\u001b[39mBoundingBoxes(read_xml(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_paths[idx]), \n\u001b[0;32m     37\u001b[0m                                 \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXYXY\u001b[39m\u001b[38;5;124m\"\u001b[39m, canvas_size\u001b[38;5;241m=\u001b[39mimage\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Generate region proposals with selective search\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     regions \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mbox_convert(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mselective_search\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_rects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxywh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxyxy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m     region_boxes \u001b[38;5;241m=\u001b[39m tv_tensors\u001b[38;5;241m.\u001b[39mBoundingBoxes(regions, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXYXY\u001b[39m\u001b[38;5;124m\"\u001b[39m, canvas_size\u001b[38;5;241m=\u001b[39mimage\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Calculate IoU between region proposals and ground-truth boxes\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 38\u001b[0m, in \u001b[0;36mselective_search\u001b[1;34m(image_path, num_rects, quality)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     ss\u001b[38;5;241m.\u001b[39mswitchToSelectiveSearchFast()\n\u001b[1;32m---> 38\u001b[0m rects \u001b[38;5;241m=\u001b[39m \u001b[43mss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rects[:num_rects]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# CNN Model\n",
    "model = Network()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Set batch size\n",
    "mini_batch = 32 # Since processing the image and labels we might end up with 200+ batches, so I make a mini batch to limit vram usage.\n",
    "epochs = 1  \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  \n",
    "    \n",
    "    total_loss = 0.0  \n",
    "    total_acc = 0.0  \n",
    "    \n",
    "    for images, boxes, regions, selected_regions, region_labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\"):\n",
    "        \n",
    "        cropped_resized_regions = process_images(images, selected_regions, target_size=(224, 224))\n",
    "        flattened_labels = process_label(images, selected_regions, region_labels)\n",
    "\n",
    "        # Skip batch if no valid regions were found in any image\n",
    "        if cropped_resized_regions.size(0) == 0 or flattened_labels.size(0) == 0:\n",
    "            continue\n",
    "\n",
    "        # Only one valid region found mean BatchNorm will fail.\n",
    "        if cropped_resized_regions.size(0) == 1:\n",
    "            continue\n",
    "        cropped_resized_regions = cropped_resized_regions.to(device)\n",
    "        flattened_labels = flattened_labels.to(device)\n",
    "\n",
    "        # Check if the batch is smaller than mini_batch\n",
    "        if len(cropped_resized_regions) < mini_batch:\n",
    "            # If only 1 region, don't split, just use the region directly\n",
    "            image_batches = [cropped_resized_regions]\n",
    "            label_batches = [flattened_labels]\n",
    "        else:\n",
    "            image_batches = torch.split(cropped_resized_regions, mini_batch)\n",
    "            label_batches = torch.split(flattened_labels, mini_batch)\n",
    "\n",
    "        # Loop through mini-batches\n",
    "        for image_batch, label_batch in zip(image_batches, label_batches):\n",
    "            \n",
    "            # some batches have no things in, which I pnnder is due to Selective Search threshold, so I skip over these for now.\n",
    "            if image_batch.size(0) == 0:\n",
    "                continue\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(image_batch)\n",
    "\n",
    "            loss = criterion(outputs, label_batch.float()) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # test stuff for now\n",
    "            # predicted = torch.sigmoid(outputs) > 0.5  \n",
    "            # acc = (predicted == label_batch).float().mean()\n",
    "            # total_acc += acc.item()\n",
    "            \n",
    "\n",
    "    # Print the average loss and accuracy for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    # avg_acc = total_acc / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}\")\n",
    "    #print(f\"Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}, Avg Accuracy: {avg_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/34 [06:01<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     61\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 63\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Average loss for the epoch\u001b[39;00m\n\u001b[0;32m     66\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pre-trained model setup\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 1)  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "mini_batch = 32\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for images, boxes, regions, selected_regions, region_labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\"):\n",
    "        \n",
    "        cropped_resized_regions = process_images(images, selected_regions, target_size=(224, 224))\n",
    "\n",
    "        flattened_labels = process_label(images, selected_regions, region_labels)\n",
    "\n",
    "        if cropped_resized_regions.size(0) == 0 or flattened_labels.size(0) == 0:\n",
    "            continue\n",
    "\n",
    "        if cropped_resized_regions.size(0) == 1:\n",
    "            continue\n",
    "        cropped_resized_regions = cropped_resized_regions.to(device)\n",
    "        flattened_labels = flattened_labels.to(device)\n",
    "\n",
    "        if len(cropped_resized_regions) < mini_batch:\n",
    "            image_batches = [cropped_resized_regions]\n",
    "            label_batches = [flattened_labels]\n",
    "        else:\n",
    "            image_batches = torch.split(cropped_resized_regions, mini_batch)\n",
    "            label_batches = torch.split(flattened_labels, mini_batch)\n",
    "\n",
    "        for image_batch, label_batch in zip(image_batches, label_batches):\n",
    "            if image_batch.size(0) == 0:\n",
    "                continue\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(image_batch)\n",
    "\n",
    "            loss = criterion(outputs.view(-1), label_batch.float())  # Flatten outputs to match target shape\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
