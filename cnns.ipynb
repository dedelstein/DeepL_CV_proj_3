{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Potholes/annotated-images/'\n",
    "splits = 'Potholes/splits.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.ops import RoIAlign\n",
    "from torchvision import models\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import functional as Fs\n",
    "from torchvision import transforms as v2\n",
    "import torchvision.ops as ops\n",
    "import torchvision.transforms.functional as Fv\n",
    "from torchvision import tv_tensors\n",
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.ops as ops\n",
    "from xml.etree import ElementTree as ET\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.io import decode_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# speed-up using multithreads\n",
    "cv2.setUseOptimized(True)\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def selective_search(image_path, num_rects, quality=True):\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    ss.setBaseImage(image)\n",
    "    \n",
    "    if quality:\n",
    "        ss.switchToSelectiveSearchQuality()\n",
    "    else:\n",
    "        ss.switchToSelectiveSearchFast()\n",
    "        \n",
    "    rects = ss.process()\n",
    "\n",
    "    return rects[:num_rects]\n",
    "\n",
    "def show_selective_search(image, rects):\n",
    "    imOut = image.copy()\n",
    "\n",
    "    # itereate over all the region proposals\n",
    "    for _, rect in enumerate(rects):\n",
    "        # draw rectangle for region proposal\n",
    "        x, y, w, h = rect\n",
    "        color = list(np.random.random(size=3) * 256)\n",
    "        cv2.rectangle(imOut, (x, y), (x+w, y+h), color, 2, cv2.LINE_AA)\n",
    "\n",
    "    plt.imshow(imOut[...,::-1])\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "def read_xml(path: str) -> list:  \n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    obj_list = []\n",
    "\n",
    "    for obj in root.iter('object'):\n",
    "\n",
    "        ymin = int(obj.find(\"bndbox/ymin\").text)\n",
    "        xmin = int(obj.find(\"bndbox/xmin\").text)\n",
    "        ymax = int(obj.find(\"bndbox/ymax\").text)\n",
    "        xmax = int(obj.find(\"bndbox/xmax\").text)\n",
    "\n",
    "        bbox = (xmin, ymin, xmax, ymax)\n",
    "        obj_list.append(bbox)\n",
    "    \n",
    "    return obj_list\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def visualize_boxes(images, annotations):\n",
    "    all_images = []\n",
    "    all_overlay = []\n",
    "\n",
    "    for image, annotation in zip(images, annotations):\n",
    "        \n",
    "        overlay = draw_bounding_boxes(image, annotation, width=2)\n",
    "        all_images.append(image)\n",
    "        all_overlay.append(overlay)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(all_images), 2, figsize=(10, len(all_images) * 5))\n",
    "    \n",
    "    for idx, image in enumerate(all_images):\n",
    "        axes[idx, 0].imshow(image.permute(1,2,0))\n",
    "        axes[idx, 0].axis('off')\n",
    "\n",
    "        axes[idx, 1].imshow(all_overlay[idx].permute(1,2,0))\n",
    "        axes[idx, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.02, hspace=0.02)\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Pretrained R-CNN\n",
    "class RCNN(nn.Module):\n",
    "    def __init__(self, num_classes, backbone='resnet50', pretrained=False):\n",
    "        super(RCNN, self).__init__()\n",
    "        \n",
    "        # Resnet50 as backbone for the pretrained R-CNN model\n",
    "        if backbone == 'resnet50':\n",
    "            resnet = models.resnet50(pretrained=pretrained)\n",
    "            self.backbone = nn.Sequential(*list(resnet.children())[:-2])  # Removing last FC layer\n",
    "            \n",
    "        # feature map size from backbone for RoI Pooling\n",
    "        self.output_channels = 2048 \n",
    "        \n",
    "        # RoI Pooling Layer\n",
    "        self.roi_pool = RoIAlign((7, 7), spatial_scale=1.0, sampling_ratio=-1) \n",
    "        \n",
    "        # Classification Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.output_channels * 7 * 7, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_classes)  \n",
    "        )\n",
    "        \n",
    "        # Bounding Box Regressor Head\n",
    "        self.bbox_regressor = nn.Sequential(\n",
    "            nn.Linear(self.output_channels * 7 * 7, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 4)  \n",
    "        )\n",
    "    \n",
    "    def forward(self, images, rois):\n",
    "        feature_maps = self.backbone(images)\n",
    "        roi_features = self.roi_pool(feature_maps, rois)\n",
    "        roi_features = roi_features.view(roi_features.size(0), -1)\n",
    "        class_logits = self.classifier(roi_features)\n",
    "        bbox = self.bbox_regressor(roi_features)\n",
    "        return class_logits, bbox\n",
    "\n",
    "\n",
    "# Used as backbone\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Define custom convolutional layers as per the framework\n",
    "        self.convolutional1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.convolutional2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.convolutional3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.convolutional4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutional1(x)\n",
    "        x = self.convolutional2(x)\n",
    "        x = self.convolutional3(x)\n",
    "        x = self.convolutional4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class homebrew_RCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(homebrew_RCNN, self).__init__()\n",
    "        \n",
    "        # Use custom backbone\n",
    "        self.backbone = CNN()\n",
    "        self.output_channels = 512  # Output channels of the last convolutional layer\n",
    "\n",
    "        # RoI Pooling Layer\n",
    "        self.roi_pool = RoIAlign((7, 7), spatial_scale=1.0, sampling_ratio=-1)\n",
    "        \n",
    "        # Classification Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.output_channels * 7 * 7, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_classes) \n",
    "        )\n",
    "        \n",
    "        # Bounding Box Regressor Head\n",
    "        self.bbox_regressor = nn.Sequential(\n",
    "            nn.Linear(self.output_channels * 7 * 7, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 4)  # Bounding box values\n",
    "        )\n",
    "    \n",
    "    def forward(self, images, rois):\n",
    "        # Extract feature maps from the backbone\n",
    "        feature_maps = self.backbone(images)\n",
    "        \n",
    "        # Apply RoI Pooling on feature maps using the provided rois\n",
    "        roi_features = self.roi_pool(feature_maps, rois)\n",
    "        roi_features = roi_features.view(roi_features.size(0), -1)\n",
    "        \n",
    "        # Get classification logits and bounding box predictions\n",
    "        class_logits = self.classifier(roi_features)\n",
    "        bbox= self.bbox_regressor(roi_features)\n",
    "        \n",
    "        return class_logits, bbox\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The current dataloader \"works\" in that the R-CNN models can be trained,\n",
    "however, its not being done exactly as the lecture says because I couldn't get it to run with that in mind. \n",
    "Another problem is that I couldn't pass the images directly into the R-CNN without resizing the images so they are the same size. \n",
    "Resizing the images caused the bounding boxes to be off target when visualizing the images, so this has to be accounted for. \n",
    "\"\"\"\n",
    "\n",
    "class Pothole_Dataloader(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, num_rects=1000, pos_thresh=0.5, neg_thresh=0.3, size=256, val=False, device=\"cpu\"):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.num_rects = num_rects\n",
    "        self.pos_thresh = pos_thresh\n",
    "        self.neg_thresh = neg_thresh\n",
    "        self.size = size\n",
    "        self.val = val\n",
    "        self.device = device\n",
    "\n",
    "        # Define data transformations\n",
    "        self.transforms = v2.Compose([\n",
    "            v2.RandomHorizontalFlip(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Decode image and bounding boxes\n",
    "        image = decode_image(self.image_paths[idx])\n",
    "        original_size = image.shape[-2:]  # (H, W)\n",
    "\n",
    "        # Resize the image\n",
    "        image = Fv.resize(image, [self.size, self.size])\n",
    "\n",
    "        # Adjust the bounding box coordinates to the resized image dimensions\n",
    "        boxes = read_xml(self.mask_paths[idx])  # Assuming boxes are loaded as [[x1, y1, x2, y2], ...]\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # Calculate scaling factors for width and height\n",
    "        scale_x, scale_y = self.size / original_size[1], self.size / original_size[0]\n",
    "\n",
    "        # Scale the bounding box coordinates\n",
    "        boxes[:, [0, 2]] *= scale_x  # Scale x1, x2\n",
    "        boxes[:, [1, 3]] *= scale_y  # Scale y1, y2\n",
    "\n",
    "        if not self.val:\n",
    "            # Select regions of interest with selective search and convert to bounding boxes\n",
    "            regions = ops.box_convert(\n",
    "                torch.tensor(selective_search(self.image_paths[idx], num_rects=self.num_rects, quality=False)).float(),\n",
    "                \"xywh\", \"xyxy\"\n",
    "            )\n",
    "\n",
    "            # Scale the region proposals\n",
    "            regions[:, [0, 2]] *= scale_x\n",
    "            regions[:, [1, 3]] *= scale_y\n",
    "\n",
    "            # Calculate IoUs between region proposals and ground-truth boxes\n",
    "            ious = ops.box_iou(regions.to(self.device), boxes.to(self.device))\n",
    "\n",
    "            # Select proposals with IoU >= pos_thresh as positive samples\n",
    "            pos_indices = (ious.max(dim=1)[0] >= self.pos_thresh).nonzero(as_tuple=True)[0]\n",
    "            pos_samples = regions[pos_indices]\n",
    "\n",
    "            # Select proposals with IoU < neg_thresh as background samples\n",
    "            bg_indices = (ious.max(dim=1)[0] < self.neg_thresh).nonzero(as_tuple=True)[0]\n",
    "            num_bg = int(len(pos_samples) * 4)  # Background samples 80% of total proposals\n",
    "            bg_samples = regions[bg_indices[:num_bg]]\n",
    "\n",
    "            # Concatenate positive and background samples\n",
    "            selected_regions = torch.cat([pos_samples, bg_samples], dim=0)\n",
    "            region_labels = torch.cat([torch.ones(len(pos_samples)), torch.zeros(num_bg)])\n",
    "\n",
    "            # Apply transformations only to the image\n",
    "            image = self.transforms(image)\n",
    "            \n",
    "            return image, boxes, regions, selected_regions, region_labels\n",
    "        else:\n",
    "            regions = selected_regions = region_labels = []\n",
    "\n",
    "        return image, boxes, regions, selected_regions, region_labels\n",
    "    \n",
    "    \n",
    "train_mask_list = [path + f for f in json.load(open(splits))['train']]\n",
    "val_mask_list = [path + f for f in json.load(open(splits))['test']]\n",
    "train_img_list = [filename.replace('xml', 'jpg') for filename in train_mask_list]\n",
    "val_img_list = [filename.replace('xml', 'jpg') for filename in val_mask_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 512\n",
    "batch_size = 32\n",
    "num_workers = 1\n",
    "\n",
    "val_img_list, test_img_list, val_mask_list, test_mask_list = train_test_split(val_img_list, val_mask_list, train_size=.5)\n",
    "\n",
    "trainset = Pothole_Dataloader(train_img_list, train_mask_list, size=size)\n",
    "valset = Pothole_Dataloader(val_img_list, val_mask_list, size=size, val=True)\n",
    "testset = Pothole_Dataloader(test_img_list, test_mask_list, size=size, val=True)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\n",
    "val_loader= DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, boxes, regions, selected_regions, region_labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #visualize_boxes(images, selected_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  12%|█▏        | 11/89 [02:18<16:19, 12.56s/batch, loss=11]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m---> 23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_regions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Convert tuple images to tensor batch\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# normalizing (caused issue by not doing it)\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\Frederik\\miniconda3\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\Frederik\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Frederik\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Frederik\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[3], line 49\u001b[0m, in \u001b[0;36mPothole_Dataloader.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     44\u001b[0m boxes[:, [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m]] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m scale_y  \u001b[38;5;66;03m# Scale y1, y2\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# Select regions of interest with selective search and convert to bounding boxes\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     regions \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mbox_convert(\n\u001b[1;32m---> 49\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mselective_search\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_rects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxywh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxyxy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m     )\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m# Scale the region proposals\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     regions[:, [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m]] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m scale_x\n",
      "Cell \u001b[1;32mIn[2], line 48\u001b[0m, in \u001b[0;36mselective_search\u001b[1;34m(image_path, num_rects, quality)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m     ss\u001b[38;5;241m.\u001b[39mswitchToSelectiveSearchFast()\n\u001b[1;32m---> 48\u001b[0m rects \u001b[38;5;241m=\u001b[39m \u001b[43mss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rects[:num_rects]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "from torchsummary import summary\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = homebrew_RCNN(num_classes=2)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "classification_loss = nn.CrossEntropyLoss() # For classification \n",
    "regression_loss = nn.SmoothL1Loss() # For regression\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "num_epochs = 2\n",
    "\n",
    "\n",
    "# Training loop that works with the current setup of dataloader for both homebrew and pretrained R-CNN. \n",
    "# It is slow training right now. \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "        for images, boxes, regions, selected_regions, region_labels in pbar:\n",
    "\n",
    "            # Convert tuple images to tensor batch\n",
    "            images = torch.stack(images).to(device)\n",
    "            images = images.float() / 255.0 # normalizing (caused issue by not doing it)\n",
    "\n",
    "            region_labels = [label.to(device) for label in region_labels]  \n",
    "\n",
    "            \"\"\"\n",
    "            process a list of ROIs and their corresponding labels from multiple images. \n",
    "            For each image:\n",
    "            It concat the image index with the ROI coordinates to create a unified representation of each ROI.\n",
    "            It then appends the newly created ROIs and their labels to separate lists for further processing.\n",
    "            \"\"\"\n",
    "            rois = []\n",
    "            roi_labels = []\n",
    "\n",
    "\n",
    "            for img_idx, (img_regions, img_labels) in enumerate(zip(selected_regions, region_labels)):\n",
    "                img_rois = torch.cat([torch.full((img_regions.size(0), 1), img_idx, dtype=torch.float32), img_regions.float()], dim=1)\n",
    "                rois.append(img_rois)\n",
    "                roi_labels.append(img_labels)\n",
    "\n",
    "            rois = torch.cat(rois, dim=0).to(device) \n",
    "            spatial_scale_factor = 1 / 32 # scale factor to adjust the ROI coordinates based on the downsampling of the feature map (RCNN)\n",
    "            rois[:, 1:] *= spatial_scale_factor # rescales the ROI coordinates\n",
    "            roi_labels = torch.cat(roi_labels, dim=0).to(device) \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            class_logits, bbox_deltas = model(images, rois)\n",
    "\n",
    "            loss_cls = classification_loss(class_logits, roi_labels.long())  # roi_labels had to be .long() didn't look into why. \n",
    "            loss_reg = regression_loss(bbox_deltas, rois[:, 1:])  # Compare to ground truth rois excluding index\n",
    "\n",
    "            total_loss = loss_cls + loss_reg\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += total_loss.item()\n",
    "\n",
    "            pbar.set_postfix(loss=running_loss / (pbar.n + 1))\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
