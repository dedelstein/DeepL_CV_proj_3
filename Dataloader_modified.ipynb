{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Potholes/annotated-images/'\n",
    "splits = 'Potholes/splits.json'\n",
    "data = 'proposals.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# speed-up using multithreads\n",
    "cv2.setUseOptimized(True)\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def selective_search(image_path, num_rects, quality=True):\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    ss.setBaseImage(image)\n",
    "\n",
    "    ss.switchToSelectiveSearchFast()\n",
    "        \n",
    "    rects = ss.process()\n",
    "\n",
    "    return rects[:num_rects]\n",
    "\n",
    "def show_selective_search(image, rects):\n",
    "    imOut = image.copy()\n",
    "\n",
    "    # itereate over all the region proposals\n",
    "    for _, rect in enumerate(rects):\n",
    "        # draw rectangle for region proposal\n",
    "        x, y, w, h = rect\n",
    "        color = list(np.random.random(size=3) * 256)\n",
    "        cv2.rectangle(imOut, (x, y), (x+w, y+h), color, 2, cv2.LINE_AA)\n",
    "\n",
    "    plt.imshow(imOut[...,::-1])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.ops as ops\n",
    "from xml.etree import ElementTree as ET\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.io import decode_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def read_xml(path: str) -> list:  \n",
    "\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    obj_list = []\n",
    "\n",
    "    for obj in root.iter('object'):\n",
    "\n",
    "        ymin = int(obj.find(\"bndbox/ymin\").text)\n",
    "        xmin = int(obj.find(\"bndbox/xmin\").text)\n",
    "        ymax = int(obj.find(\"bndbox/ymax\").text)\n",
    "        xmax = int(obj.find(\"bndbox/xmax\").text)\n",
    "\n",
    "        bbox = (xmin, ymin, xmax, ymax)\n",
    "        obj_list.append(bbox)\n",
    "    \n",
    "    return obj_list\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def visualize_boxes(images, annotations):\n",
    "    all_images = []\n",
    "    all_overlay = []\n",
    "\n",
    "    for image, annotation in zip(images, annotations):\n",
    "        \n",
    "        overlay = draw_bounding_boxes(image, annotation, width=2)\n",
    "        all_images.append(image)\n",
    "        all_overlay.append(overlay)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(all_images), 2, figsize=(10, len(all_images) * 5))\n",
    "    \n",
    "    for idx, image in enumerate(all_images):\n",
    "        axes[idx, 0].imshow(image.permute(1,2,0))\n",
    "        axes[idx, 0].axis('off')\n",
    "\n",
    "        axes[idx, 1].imshow(all_overlay[idx].permute(1,2,0))\n",
    "        axes[idx, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.02, hspace=0.02)\n",
    "    plt.show()\n",
    "\n",
    "class Pothole_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, feature_size=256, pos_thresh=.7, neg_thresh=.3, val=False, device=\"cpu\"):\n",
    "        self.data = data\n",
    "        self.size = feature_size\n",
    "        self.pos_thresh = pos_thresh\n",
    "        self.neg_thresh = neg_thresh\n",
    "        self.val = val\n",
    "        self.device = device\n",
    "\n",
    "        self.transforms = v2.Compose([\n",
    "                            v2.RandomHorizontalFlip(),\n",
    "                            v2.RandomVerticalFlip()\n",
    "                          ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        datum = self.data[idx]\n",
    "        image = decode_image(datum[0]).to(self.device)\n",
    "        ground_truths = datum[1].to(self.device)\n",
    "        regions = datum[2].to(self.device)\n",
    "\n",
    "        ious = ops.box_iou(regions, ground_truths)\n",
    "\n",
    "        # Select proposals with IoU >= pos_thresh as positive samples\n",
    "        pos_indices = (ious.max(dim=1)[0] >= self.pos_thresh).nonzero(as_tuple=True)[0]\n",
    "        pos_samples = regions[pos_indices]\n",
    "\n",
    "        # Select proposals with IoU < neg_thresh as background samples\n",
    "        bg_indices = (ious.max(dim=1)[0] < self.neg_thresh).nonzero(as_tuple=True)[0]\n",
    "        num_bg = int(len(pos_samples) * 4)  # Make background samples 80% of total proposals\n",
    "        bg_samples = regions[bg_indices[:num_bg]]\n",
    "        \n",
    "        # Concatenate positive and background samples \n",
    "        selected_regions = torch.cat([pos_samples, bg_samples], dim=0) \n",
    "        region_labels = torch.cat([torch.ones(len(pos_samples), device=self.device), torch.zeros(num_bg, device=self.device)])\n",
    "\n",
    "        # Collect and stack image elements\n",
    "        batched_image = image.unsqueeze(0).float()\n",
    "        # 2. Format boxes for ROI Align\n",
    "        # ROI Align expects boxes in format (batch_idx, x1, y1, x2, y2)\n",
    "        N = len(selected_regions)\n",
    "        # Add batch index (0) as first column\n",
    "        batched_regions = torch.zeros(N, 5, device=self.device)\n",
    "        batched_regions[:, 1:] = selected_regions  # Copy x1, y1, x2, y2        \n",
    "\n",
    "        # 3. Extract ROIs using ROI Align\n",
    "        rois = ops.roi_align(\n",
    "            input=batched_image,        # (1, C, H, W)\n",
    "            boxes=batched_regions,        # (N, 5)\n",
    "            output_size=(self.size, self.size),\n",
    "            spatial_scale=1.0,          # No scaling if boxes are in absolute coords\n",
    "            aligned=True                # Better alignment with original image\n",
    "        )\n",
    "        \n",
    "        if not self.val:\n",
    "            rois = self.transforms(rois)\n",
    "        \n",
    "        return image, ground_truths, regions, selected_regions, region_labels, rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = 512\n",
    "batch_size = 6\n",
    "num_workers = 1\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = 'proposals.pkl'\n",
    "with open(pkl_file, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "    \n",
    "train_len = 532\n",
    "train_data = data[:train_len]\n",
    "val_data, test_data = train_test_split(data[train_len:], train_size=.5)\n",
    "\n",
    "trainset = Pothole_Dataset(train_data, feature_size=feature_size, device=device)\n",
    "valset = Pothole_Dataset(val_data, feature_size=feature_size, device=device, val=True)\n",
    "testset = Pothole_Dataset(test_data, feature_size=feature_size, device=device, val=True)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\n",
    "val_loader= DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, ground_truths, regions, selected_regions, region_labels, rois = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_boxes(images, regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_boxes(images, selected_regions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
